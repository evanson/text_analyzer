import re

def tokenize(text):
    words = []
    regex = re.compile("[a-zA-z']+")
    tokens = text.split()
    for token in tokens:
        m = regex.match(token)
        plain_word = m.group()
        words.append(plain_word.lower())
    return words


